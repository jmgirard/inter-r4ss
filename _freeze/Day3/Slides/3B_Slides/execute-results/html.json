{
  "hash": "6ff2d0d477ff54a748590e2ae290ae03",
  "result": {
    "markdown": "---\nformat: \n  revealjs:\n    css: ../../styles.css\n    slide-number: true\n    show-slide-number: all\n    progress: true\n    history: true\n    hash-type: number\n    theme: default\n    code-block-background: true\n    highlight-style: github\n    code-link: false\n    code-copy: true\n    controls: true\n    pagetitle: \"Inter R4SS Day 3B\"\n    author-meta: \"Jeffrey Girard\"\n    date-meta: \"2023-06-07\"\n---\n\n\n::: {.my-title}\n# [Intermediate R]{.blue2} <br />for Social Scientists\n\n::: {.my-grey}\n[Workshop Day 3B | 2023-06-07]{}<br />\n[Jeffrey M. Girard | Pitt Methods]{}\n:::\n\n![](../../img/proud2_2780E3.svg){.absolute bottom=0 right=0 width=400}\n:::\n\n# Generalized <br />Linear Modeling\n\n## The Linear Model (LM)\n\n$$\n\\begin{align}\n\\mu_i &= b_0 + b_1 x_{1i} + \\cdots + b_m x_{mi}\\\\\ny_i &\\sim \\text{Gaussian}(\\mu_i, \\sigma)\n\\end{align}\n$$\n\n::: {.fragment .f80}\n\n- The first (structural) term is [linear]{.b .blue} (i.e., the sum of products)\n    -   It assumes that $\\mu_i$ can vary indefinitely, i.e., $y \\in (-\\infty,\\infty)$\n    \n:::\n\n::: {.fragment .f80}\n- The second (random) term describes the [shape of the residuals]{.b .blue}\n    -   It assumes they are [normally (Gaussian) distributed]{.b .green} around $\\mu_i$\n    -   It assumes they have [constant variance]{.b .green} (i.e., there is only one $\\sigma$)\n    \n:::\n\n## The Generalized LM (GLM)\n\n::: {.f80}\n- What happens when these assumptions are violated?\n    -   Our parameters and inferences will be biased\n    -   Our model may make impossible predictions\n\n:::\n\n::: {.fragment}\n\n::: {.f80}\n- GLM addresses this issue by making two changes\n    -   Adds a [family]{.b .blue}, i.e., specifies a different shape of the residuals\n    -   Adds a [link function]{.b .green}, i.e., transforms $y$ to vary indefinitely\n\n:::\n\n$$\n\\begin{align}\n\\color{ForestGreen}{\\text{link}}(\\mu_i) &= b_0 + b_1 x_{1i} + \\cdots + b_m x_{mi} \\\\\ny_i &\\sim \\color{blue}{\\text{family}}(\\mu_i, \\ldots)\n\\end{align}\n$$\n:::\n\n\n## Basic GLM Families\n\n- [Normal:]{.b .green} $y \\in \\mathbb{R}$, use `gaussian()` with identity link\n- [Nonnegative:]{.b .green} $y \\in \\mathbb{R}^+$, use `Gamma()` with inverse link\n- [Binary:]{.b .blue} $y \\in \\{0,1\\}$, use `binomial()` with logit/probit link\n- [Counts:]{.b .blue} $y \\in \\{0, 1, 2, ...\\}$, use `poisson()` with log link\n\n::: {.fragment .mt1}\n-   Many advanced/specialized GLM families are also available!\n:::\n\n## Binary Regression\n\n-   Binary regression is used when $y$ is binary, i.e., $0$ or $1$\n    -   *e.g., Is the patient healthy (0) or sick (1)?*\n    -   *e.g., Did the student fail (0) or pass (1)?*\n    -   *e.g., Is the attribute absent (0) or present (1)?*\n    -   *e.g., Was the coin flip heads (0) or tails (1)?*\n\n## The Binomial Family\n\n$$\nk \\sim \\text{Binomial}(n,p)\n$$\n\n- $n\\in\\{0,1,2,...\\}$ --- the number of trials\n- $p\\in[0,1]$ --- the probability of success in each trial\n- $k\\in\\{0,1,...,n\\}$ --- the number of successes\n\n::: {.fragment}\n-   This family applies to [binary]{.b .blue} cases when $n=1$\n    -   *e.g., If I flip one coin, how many landed heads?*\n    \n:::\n\n## Logistic Regression\n\n$$\n\\begin{align}\n\\text{logit}(p_i) &= b_0 + b_1 x_{1i} + \\cdots + b_m x_{mi} \\\\\ny_i &\\sim \\text{Binomial}(1, p_i)\n\\end{align}\n$$\n\n-   We can't predict $p$ directly because it is bounded, $p\\in[0,1]$\n-   The [logit link]{.b .blue} makes $p$ unbounded, $\\text{logit}(p)\\in(-\\infty,\\infty)$\n\n$$\\text{logit}(p)=\\log(\\text{Odds}(p))=\\log\\left(\\frac{p}{1-p}\\right)$$\n\n## The Logit Link\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3B_Slides_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n\n## Interpretation\n\n- Our slopes ($b_m$) will be in [logit (log-odd) units]{.b .green}\n    -   $b_m>0$ indicates increased prob., $b_m<0$ decreased\n\n::: {.fragment}\n- We can exponentiate them to convert to [Odds Ratios]{.b .blue}\n    -   If $b_1=0.3$, then $OR=e^{0.3}=1.35$ times the odds of success (or a 35% increase) when increasing $x_1$ by 1\n    -   If $b_1=-0.3$, then $OR=e^{-0.3}=0.74$ times the odds of success (or a 26% decrease) when increasing $x_1$ by 1\n\n:::\n\n## Binary Regression Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n## Load packages (after installing, if needed)\nlibrary(tidyverse)\nlibrary(easystats)\n\n## Read in some example binary data and mutate the outcome to be 0 vs. 1\ntitanic <- \n  read_csv(\"titanic.csv\") |> \n  mutate(survived = if_else(survived == \"yes\", true = 1, false = 0)) |> \n  print()\n\n## Visualize the relationship between fare and survival\nggplot(titanic, aes(x = fare, y = survived)) +\n  geom_point() +\n  geom_smooth()\n\n## Fit a basic regression using LM\nfit <- lm(\n  formula = survived ~ fare, \n  data = titanic\n)\nmodel_parameters(fit)\nmodel_parameters(fit) |> print_md() #looks better in quarto\nmodel_performance(fit)\nplot(estimate_expectation(fit))\ncheck_model(fit)\n\n## Fit a logistic regression using GLM\nfit2 <- glm(\n  formula = survived ~ fare, \n  family = binomial(link = \"logit\"), \n  data = titanic\n)\nmodel_parameters(fit2)\nmodel_parameters(fit2, exponentiate = TRUE)\nmodel_performance(fit2)\nplot(estimate_link(fit2))\nplot(estimate_expectation(fit2))\ncheck_model(fit2)\n\n## Estimate the model's expectation for specific values of the predictor(s)\nestimate_expectation(fit2, tibble(fare = c(10, 50, 100)))\n\n## Note: To fit a probit binary regression, just use binomial(link = \"probit\")\n```\n:::\n\n\n\n## Count Regression\n\n- Count regression is used when $y$ is a [whole number]{.b .green}\n\n::: {.fragment}\n- Counts can be [bounded]{.b .green} (i.e., have a maximum number)\n    - *e.g., how many answers on the test were correct?*\n    - For these, we can re-use the **Binomial** family $(n>0)$\n\n:::\n\n::: {.fragment}\n- Counts can also be [unbounded]{.b .blue} (i.e., no maximum)\n    - *e.g., how many hospitalizations in the last 10 years?*\n    - For these, we can use **Poisson** or **Negative Binomial**\n    \n:::\n\n\n## The Poisson Family\n\n$$\nk \\sim \\text{Poisson}(\\lambda)\n$$\n\n- $\\lambda\\in(0,\\infty)$ --- occurrence rate\n- $k\\in\\mathbb{N}_0$ --- number of occurrences\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3B_Slides_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n\n## Poisson Regression\n$$\n\\begin{align}\n\\log(\\lambda_i) &= b_0 + b_1 x_{1i} + \\cdots + b_m x_{mi} \\\\\ny_i &\\sim \\text{Poisson}(\\lambda_i)\n\\end{align}\n$$\n\n-   We can't predict $\\lambda$ directly; it is bounded $\\lambda\\in(0,\\infty)$\n-   The [log link]{.b .blue} makes $\\lambda$ unbounded, $\\log(\\lambda)\\in(-\\infty,\\infty)$\n\n## Interpretation\n\n- Our slopes ($b_m$) will be in [log units]{.b .green}\n    -   $b_m>0$ indicates increased rate, $b_m<0$  decreased\n\n::: {.fragment}\n- We can exponentiate to convert to [Incidence Rate Ratios]{.b .blue}\n    -   If $b_1=0.3$, then $IRR=e^{0.3}=1.35$ times the incidence rate (a 35% increase) when increasing $x_1$ by 1\n    -   If $b_1=-0.3$, then $IRR=e^{-0.3}=0.74$ times the incidence rate (a 26% decrease) when increasing $x_1$ by 1\n\n:::\n\n## Count Regression Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# Read in some example count data\ndisc <- read_csv(\"discoveries.csv\")\ndisc\n\n# Visualize the relationship between year and discovery count\nggplot(disc, aes(x = year, y = count)) + \n  geom_point() +\n  geom_smooth()\n\n# Fit a linear regression using LM\nfit <- lm(\n  formula = count ~ poly(year, degree = 2), \n  data = disc\n)\nmodel_parameters(fit)\nmodel_performance(fit)\nplot(estimate_relation(fit))\ncheck_model(fit)\n\n# Fit a poisson regression using GLM\nfit2 <- glm(\n  formula = count ~ poly(year, degree = 2),\n  family = poisson(link = \"log\"), \n  data = disc\n)\nmodel_parameters(fit2)\nmodel_parameters(fit2, exponentiate = TRUE)\nmodel_performance(fit2)\nplot(estimate_relation(fit2))\n\n# Advanced: Overdispersion and quasi-poisson\ncheck_overdispersion(fit2)\nfit3 <- glm(\n  count ~ poly(year, degree = 2), \n  family = quasipoisson(link = \"log\"), \n  data = disc\n)\nmodel_parameters(fit3)\nmodel_parameters(fit3, exponentiate = TRUE)\nplot(estimate_relation(fit3))\n\n# Compare parameters between poisson and quasi-poisson\ncompare_parameters(fit2, fit3, select = \"ci_p2\")\n```\n:::\n\n\n# Multilevel (Mixed Effects) Modeling\n\n## Conceptual Overview {.smaller}\n\n- LM and GLM both assume [independent residuals]{.b .blue}\n    - This is violated by clustered and longitudinal data\n    - The estimated SEs will be **biased** (usually too small)\n\n::: {.fragment}\n- [Clustered data]{.b .green} has a higher level of organization\n    - *e.g., students from same classroom are more similar*\n    - *e.g., participants from same country are more similar*\n    \n:::\n\n::: {.fragment}\n- [Longitudinal data]{.b .green} is repeated sampled from same source\n    - *e.g., each participant responds to multiple trials*\n    - *e.g., the global temperature is measured every week*\n\n:::\n\n## Cluster-Robust Standard Errors {.smaller}\n\n- The basic OLS standard errors will be biased with clustered data\n- CR-SEs use a statistical procedure to try to correct this bias\n    -   This is convenient when clustering is just a \"nuisance\"\n\n::: {.fragment}\n1. Estimate the regression coefficients using OLS (these are unchanged)\n1. Estimate the CR-SEs using a more complex formula, e.g., based on clusters $J$\n1. Interpret the coefficients and CR-SEs as you would in a basic LM or GLM\n\n:::\n\n::: {.fragment .tc}\n$$\n\\begin{align}\nVar^{OLS}(\\hat{b})&=\\sigma^2(\\mathbf{X}^\\intercal\\mathbf{X})^{-1} \\\\\nVar^{CR}(\\hat{b})&=(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\sum_{j=1}^J(\\mathbf{X}_j^\\intercal e_je_j^\\intercal \\mathbf{X}_j)(\\mathbf{X}^\\intercal \\mathbf{X})^{-1}\n\\end{align}\n$$\n\n*In the latter, we sum residuals by cluster instead of observation*\n:::\n\n## Cluster-Robust Standard Errors\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(sandwich)\nmlmath <- read_csv(\"mlmath.csv\")\nmlmath\n\nmlmath |> count(school)\nmlmath |> count(public)\n\n# Fit LM ignoring clustering within schools\n\nfit <- lm(\n  formula = math ~ 1 + homework,\n  data = mlmath\n)\nmodel_parameters(fit)\nmodel_performance(fit)\nestimate_relation(fit) |> plot()\n\n# Estimate cluster-robust standard errors\n\nmodel_parameters(\n  fit, \n  vcov = \"vcovCL\", \n  vcov_args = list(type = \"HC1\", cluster = mlmath$school)\n)\nestimate_relation(\n  fit,\n  vcov = \"vcovCL\", \n  vcov_args = list(type = \"HC1\", cluster = mlmath$school)\n) |> \n  plot()\n```\n:::\n\n\n## Simpson's Paradox {.smaller}\n\nHowever, reaching the right conclusions may require knowing the clusters...\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3B_Slides_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n## Simpson's Paradox {.smaller}\n\nHere it looks like having **worse** symptoms predicts a ***better*** prognosis...\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3B_Slides_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n## Simpson's Paradox {.smaller}\n\nBut this data actually comes from three different hospitals!\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3B_Slides_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n## Simpson's Paradox {.smaller}\n\nWithin each hospital, the relationship is the opposite!\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3B_Slides_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n## Levels of Clustering {.smaller}\n\n- We can think and talk about clustered data as having [multiple levels]{.b .blue} (hence MLM)\n\n::: {.fragment}\n- We number levels from $1$ to $M$ where lower numbers represent more specific levels\n    - *e.g., Data is collected from students (L1) nested within classrooms (L2)*\n    - *e.g., Data is collected from students (L1) in classrooms (L2) in schools (L3)*\n    - *e.g., Data is collected from timepoints (L1) nested within participants (L2)*\n    - *e.g., Data is collected from timepoints (L1) in participants (L2) in countries (L3)*\n\n:::\n\n::: {.fragment}\n- What is and isn't considered a level?\n    - At each level, we are sampling from a broader population\n    - Thus, levels need to be *random* variables (and are usually discrete)\n    - *If we repeated the study, would the variable's values be the same?*\n    \n:::\n\n## Variables at Different Levels\n\n- We can have variables that vary at (or describe) each level\n    - L1: each **student**'s `age`, `sex`, and `test_score`\n    - L2: each **classroom**'s `subject` and `n_students`\n    - L3: each **school**'s `budget` and `charter_status`\n\n::: {.fragment}\n- In standard MLM, the outcome/DV has to be on L1\n    - But we can still use higher level predictors\n    - We can even have cross-level interactions\n\n:::\n\n## Mixed Effects (Multilevel) Modeling {.smaller}\n\nWhen we have multiple clusters of data, we have several options for fitting linear models\n\n1. **Complete Pooling:** Fit a single line to all the data, ignoring clustering\n\n1. **No Pooling:** Fit a separate line for each cluster, ignoring any similarity\n\n1. **Partial Pooling:** Fit a *distribution* of similar lines to the data, one for each cluster\n\n::: {.fragment .mt2}\nMultilevel modeling uses the partial pooling approach, which yields two types of effects\n\n- [Fixed Effects]{.b .green} either describe an effect assumed to be the same for all clusters, <br />or describe the center of an effect's distribution across clusters\n\n- [Random Effects]{.b .blue} describe how each cluster deviates from the fixed effect and therefore create the distribution across clusters (can be estimated or not)\n\n:::\n\n## Example Models {.smaller}\n\n0. **Null Model**\n    - Each cluster (school) has its own intercept (average math score)\n    - There are no predictors/slopes added (fixed or random)\n\n::: {.fragment}\n1. **Random Intercepts (RI) Model**\n    - Each cluster (school) has its own intercept (average math score)\n    - The slope (homework effect) is fixed and applies to all clusters (schools)\n    \n:::\n\n## Example Models {.smaller}\n\n2. **Random Intercepts and Slopes (RIS) Model**\n    - Each cluster (school) has its own intercept (average math score)\n    - Each cluster (school) has its own slope (homework effect)\n\n::: {.fragment}\n3. **Cross-Level Interaction Model**\n    - An L1 (student) outcome will be regressed on an L2 (school) predictor\n    - An L1 (student) predictor will interact with an L2 (school) predictor\n    \n:::\n\n## Null and RI Models Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nlibrary(lme4)\nlibrary(merDeriv)\nlibrary(ggeffects)\n\n# ==============================================================================\n# Fit and examine null model\n\nfit_null <- lmer(\n  formula = math ~ 1 + (1 | school),\n  data = mlmath\n)\nmodel_parameters(fit_null) |> print_md()\nmodel_performance(fit_null) |> print_md()\n\n# ==============================================================================\n# Fit and examine random intercepts model\n\nfit_ri <- lmer(\n  formula = math ~ 1 + homework + (1 | school), \n  data = mlmath\n)\nmodel_parameters(fit_ri) |> print_md()\nmodel_performance(fit_ri) |> print_md()\n\nplot(estimate_relation(fit_ri))\n\n## random effects (deviations from the fixed effect)\nestimate_grouplevel(fit_ri) |> print_md() \nestimate_grouplevel(fit_ri) |> plot()\n\n## fixed effects + random effects (deviation)\nestimate_grouplevel(fit_ri, type = \"total\") |> print_md() \n\n## plot the line for each cluster (school)\nggpredict(fit_ri, c(\"homework\", \"school\"), type = \"random\") |> \n  plot(colors = 1:10, ci = FALSE)\n```\n:::\n\n\n## RIS and CLI Models Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# ==============================================================================\n# Fit and examine random intercepts and slopes model\n\nfit_ris <- lmer(\n  formula = math ~ 1 + homework + (1 + homework | school),\n  data = mlmath\n)\nmodel_parameters(fit_ris) |> print_md()\nmodel_performance(fit_ris) |> print_md()\n\nplot(estimate_relation(fit_ris))\n\n## random effects (deviations from the fixed effect)\nestimate_grouplevel(fit_ris) |> print_md() \nestimate_grouplevel(fit_ris) |> plot()\n\n## fixed effects + random effects (deviation)\nestimate_grouplevel(fit_ris, type = \"total\") |> print_md() \n\n## plot the line for each cluster (school)\nggpredict(fit_ris, c(\"homework\", \"school\"), type = \"random\") |> \n  plot(colors = 1:10, ci = FALSE)\n\n## turn refit off (use REML) to compare models with the same fixed effects\nanova(fit_ri, fit_ris, refit = FALSE)\n\n# ==============================================================================\n# Fit and examine the cross-level interaction model\n\nfit_cli <- lmer(\n  formula = math ~ 1 + homework * public + (1 + homework | school),\n  data = mlmath\n)\nmodel_parameters(fit_cli) |> print_md()\nmodel_performance(fit_cli) |> print_md()\n\nplot(estimate_relation(fit_cli))\n\n## estimate simple slopes\nestimate_slopes(fit_cli, trend = \"homework\", at = \"public\") |> print_md()\n\n## Turn refit on (use ML) to compare models with different fixed effects\nanova(fit_ris, fit_cli, refit = TRUE)\n```\n:::\n\n\n## Centering\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# Grand mean centering (good for level-2 variables)\n\nmlmath2 <- \n  mlmath |> \n  mutate(ratio_c = center(ratio)) |> \n  print()\n\n# Centering within clusters (good for level-1 variables)\n\nmlmath3 <- \n  mlmath2 |> \n  mutate(\n    homework_m = mean(homework), # cluster mean (between)\n    homework_d = homework - homework_m, # deviation from cluster mean (within)\n    .by = school\n  ) |> \n  print()\n\n## Incorporating these variables into the model\n\nfit <- lmer(\n  math ~ 1 + homework_m + homework_d + ratio_c + (1 + homework_d | school),\n  data = mlmath3\n)\nmodel_parameters(fit) |> print_md()\nmodel_performance(fit) |> print_md()\n```\n:::\n\n\n\n## Longitudinal Considerations\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# ==============================================================================\n# Load example data\n\nsoccer <- \n  read_csv(\"soccer.csv\") |> \n  mutate(across(c(Code, Played), factor)) |> \n  print()\n\n# ==============================================================================\n# Fit null model and examine ICC\n\nfit0 <- lmer(\n  Testosterone ~ 1 + (1 | Code),\n  data = soccer\n)\nmodel_performance(fit0) |> print_md()\n\n# ==============================================================================\n# Fit random intercepts longitudinal model\n\nfit1 <- lmer(\n  Testosterone ~ 1 + time0 + (1 | Code),\n  data = soccer\n)\nmodel_parameters(fit1) |> print_md()\nmodel_performance(fit1) |> print_md()\n\nestimate_relation(fit1) |> plot()\n\nggpredict(fit1, c(\"time0\", \"Code\"), type = \"random\") |> \n  plot(colors = 1:25, ci = FALSE)\n\n# ==============================================================================\n# Fit random intercepts and slopes model\n\nfit2 <- lmer(\n  Testosterone ~ 1 + time0 + (1 + time0 | Code),\n  data = soccer\n)\nmodel_parameters(fit2) |> print_md()\nmodel_performance(fit2) |> print_md()\n\nggpredict(fit2, c(\"time0\", \"Code\"), type = \"random\") |> \n  plot(colors = 1:25, ci = FALSE)\n\n## Turn refit off (use REML) when comparing models with same fixed effects\nanova(fit1, fit2, refit = FALSE)\n\n# ==============================================================================\n# Fit cross-level interaction model\n\nfit3 <- lmer(\n  Testosterone ~ 1 + time0 * Played + (1 + time0 | Code),\n  data = soccer\n)\nmodel_parameters(fit3) |> print_md()\nmodel_performance(fit3) |> print_md()\n\nestimate_relation(fit3) |> plot()\n\nggpredict(fit3, c(\"time0\", \"Code\", \"Played\"), type = \"random\") |> \n  plot(colors = 1:25, ci = FALSE)\n\n## Turn refit on (use ML) when comparing models with different fixed effects\nanova(fit2, fit3, refit = TRUE)\n```\n:::\n",
    "supporting": [
      "3B_Slides_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}