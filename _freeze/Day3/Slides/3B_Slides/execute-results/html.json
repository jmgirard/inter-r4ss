{
  "hash": "f4a4e2920ab823e7b02f025d8ad04ae5",
  "result": {
    "markdown": "---\nformat: \n  revealjs:\n    css: ../../styles.css\n    slide-number: true\n    show-slide-number: all\n    progress: true\n    history: true\n    hash-type: number\n    theme: default\n    code-block-background: true\n    highlight-style: github\n    code-link: false\n    code-copy: true\n    controls: true\n    pagetitle: \"Inter R4SS Day 3B\"\n    author-meta: \"Jeffrey Girard\"\n    date-meta: \"2023-06-07\"\n---\n\n\n::: {.my-title}\n# [Intermediate R]{.blue2} <br />for Social Scientists\n\n::: {.my-grey}\n[Workshop Day 3B | 2023-06-07]{}<br />\n[Jeffrey M. Girard | Pitt Methods]{}\n:::\n\n![](../../img/proud2_2780E3.svg){.absolute bottom=0 right=0 width=400}\n:::\n\n# Generalized <br />Linear Modeling\n\n## The Linear Model (LM)\n\n$$\n\\begin{align}\n\\mu_i &= b_0 + b_1 x_{1i} + \\cdots + b_m x_{mi}\\\\\ny_i &\\sim \\text{Gaussian}(\\mu_i, \\sigma)\n\\end{align}\n$$\n\n::: {.fragment .f80}\n\n- The first (structural) term is [linear]{.b .blue} (i.e., the sum of products)\n    -   It assumes that $\\mu_i$ can vary indefinitely, i.e., $y \\in (-\\infty,\\infty)$\n    \n:::\n\n::: {.fragment .f80}\n- The second (random) term describes the [shape of the residuals]{.b .blue}\n    -   It assumes they are [normally (Gaussian) distributed]{.b .green} around $\\mu_i$\n    -   It assumes they have [constant variance]{.b .green} (i.e., there is only one $\\sigma$)\n    \n:::\n\n## The Generalized LM (GLM)\n\n::: {.f80}\n- What happens when these assumptions are violated?\n    -   Our parameters and inferences will be biased\n    -   Our model may make impossible predictions\n\n:::\n\n::: {.fragment}\n\n::: {.f80}\n- GLM addresses this issue by making two changes\n    -   Adds a [family]{.b .blue}, i.e., specifies a different shape of the residuals\n    -   Adds a [link function]{.b .green}, i.e., transforms $y$ to vary indefinitely\n\n:::\n\n$$\n\\begin{align}\n\\color{ForestGreen}{\\text{link}}(\\mu_i) &= b_0 + b_1 x_{1i} + \\cdots + b_m x_{mi} \\\\\ny_i &\\sim \\color{blue}{\\text{family}}(\\mu_i, \\ldots)\n\\end{align}\n$$\n:::\n\n\n## Basic GLM Families\n\n- [Normal:]{.b .green} $y \\in \\mathbb{R}$, use `gaussian()` with identity link\n- [Nonnegative:]{.b .green} $y \\in \\mathbb{R}^+$, use `Gamma()` with inverse link\n- [Binary:]{.b .blue} $y \\in \\{0,1\\}$, use `binomial()` with logit/probit link\n- [Counts:]{.b .blue} $y \\in \\{0, 1, 2, ...\\}$, use `poisson()` with log link\n\n::: {.fragment .mt1}\n-   Many advanced/specialized GLM families are also available!\n:::\n\n## Binary Regression\n\n-   Binary regression is used when $y$ is binary, i.e., $0$ or $1$\n    -   *e.g., Is the patient healthy (0) or sick (1)?*\n    -   *e.g., Did the student fail (0) or pass (1)?*\n    -   *e.g., Is the attribute absent (0) or present (1)?*\n    -   *e.g., Was the coin flip heads (0) or tails (1)?*\n\n## The Binomial Family\n\n$$\nk \\sim \\text{Binomial}(n,p)\n$$\n\n- $n\\in\\{0,1,2,...\\}$ --- the number of trials\n- $p\\in[0,1]$ --- the probability of success in each trial\n- $k\\in\\{0,1,...,n\\}$ --- the number of successes\n\n::: {.fragment}\n-   This family applies to [binary]{.b .blue} cases when $n=1$\n    -   *e.g., If I flip one coin, how many landed heads?*\n    \n:::\n\n## Logistic Regression\n\n$$\n\\begin{align}\n\\text{logit}(p_i) &= b_0 + b_1 x_{1i} + \\cdots + b_m x_{mi} \\\\\ny_i &\\sim \\text{Binomial}(1, p_i)\n\\end{align}\n$$\n\n-   We can't predict $p$ directly because it is bounded, $p\\in[0,1]$\n-   The [logit link]{.b .blue} makes $p$ unbounded, $\\text{logit}(p)\\in(-\\infty,\\infty)$\n\n$$\\text{logit}(p)=\\log(\\text{Odds}(p))=\\log\\left(\\frac{p}{1-p}\\right)$$\n\n## The Logit Link\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3B_Slides_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n\n## Interpretation\n\n- Our slopes ($b_m$) will be in [logit (log-odd) units]{.b .green}\n    -   $b_m>0$ indicates increased prob., $b_m<0$ decreased\n\n::: {.fragment}\n- We can exponentiate them to convert to [Odds Ratios]{.b .blue}\n    -   If $b_1=0.3$, then $OR=e^{0.3}=1.35$ times the odds of success (or a 35% increase) when increasing $x_1$ by 1\n    -   If $b_1=-0.3$, then $OR=e^{-0.3}=0.74$ times the odds of success (or a 26% decrease) when increasing $x_1$ by 1\n\n:::\n\n## Binary Regression Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n## Load packages (after installing, if needed)\nlibrary(tidyverse)\nlibrary(easystats)\n\n## Read in some example binary data and mutate the outcome to be 0 vs. 1\ntitanic <- \n  read_csv(\"titanic.csv\") |> \n  mutate(survived = if_else(survived == \"yes\", true = 1, false = 0)) |> \n  print()\n\n## Visualize the relationship between fare and survival\nggplot(titanic, aes(x = fare, y = survived)) +\n  geom_point() +\n  geom_smooth()\n\n## Fit a basic regression using LM\nfit <- lm(\n  formula = survived ~ fare, \n  data = titanic\n)\nmodel_parameters(fit)\nmodel_performance(fit)\nplot(estimate_expectation(fit))\ncheck_model(fit)\n\n## Fit a logistic regression using GLM\nfit2 <- glm(\n  formula = survived ~ fare, \n  family = binomial(link = \"logit\"), \n  data = titanic\n)\nmodel_parameters(fit2)\nmodel_parameters(fit2, exponentiate = TRUE)\nmodel_performance(fit2)\nplot(estimate_link(fit2))\nplot(estimate_expectation(fit2))\ncheck_model(fit2)\n\n## Estimate the model's expectation for specific values of the predictor(s)\nestimate_expectation(fit2, tibble(fare = c(10, 50, 100)))\n\n## Note: To fit a probit binary regression, just use binomial(link = \"probit\")\n```\n:::\n\n\n\n## Count Regression\n\n- Count regression is used when $y$ is a [whole number]{.b .green}\n\n::: {.fragment}\n- Counts can be [bounded]{.b .green} (i.e., have a maximum number)\n    - *e.g., how many answers on the test were correct?*\n    - For these, we can re-use the **Binomial** family $(n>0)$\n\n:::\n\n::: {.fragment}\n- Counts can also be [unbounded]{.b .blue} (i.e., no maximum)\n    - *e.g., how many hospitalizations in the last 10 years?*\n    - For these, we can use **Poisson** or **Negative Binomial**\n    \n:::\n\n\n## The Poisson Family\n\n$$\nk \\sim \\text{Poisson}(\\lambda)\n$$\n\n- $\\lambda\\in(0,\\infty)$ --- occurrence rate\n- $k\\in\\mathbb{N}_0$ --- number of occurrences\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3B_Slides_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n\n## Poisson Regression\n$$\n\\begin{align}\n\\log(\\lambda_i) &= b_0 + b_1 x_{1i} + \\cdots + b_m x_{mi} \\\\\ny_i &\\sim \\text{Poisson}(\\lambda_i)\n\\end{align}\n$$\n\n-   We can't predict $\\lambda$ directly; it is bounded $\\lambda\\in(0,\\infty)$\n-   The [log link]{.b .blue} makes $\\lambda$ unbounded, $\\log(\\lambda)\\in(-\\infty,\\infty)$\n\n## Interpretation\n\n- Our slopes ($b_m$) will be in [log units]{.b .green}\n    -   $b_m>0$ indicates increased rate, $b_m<0$  decreased\n\n::: {.fragment}\n- We can exponentiate to convert to [Incidence Rate Ratios]{.b .blue}\n    -   If $b_1=0.3$, then $IRR=e^{0.3}=1.35$ times the incidence rate (a 35% increase) when increasing $x_1$ by 1\n    -   If $b_1=-0.3$, then $IRR=e^{-0.3}=0.74$ times the incidence rate (a 26% decrease) when increasing $x_1$ by 1\n\n:::\n\n## Count Regression Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# Read in some example count data\ndisc <- read_csv(\"discoveries.csv\")\ndisc\n\n# Visualize the relationship between year and discovery count\nggplot(disc, aes(x = year, y = count)) + \n  geom_point() +\n  geom_smooth()\n\n# Fit a linear regression using LM\nfit <- lm(\n  formula = count ~ poly(year, degree = 2), \n  data = disc\n)\nmodel_parameters(fit)\nmodel_performance(fit)\nplot(estimate_relation(fit))\ncheck_model(fit)\n\n# Fit a poisson regression using GLM\nfit2 <- glm(\n  formula = count ~ poly(year, degree = 2),\n  family = poisson(link = \"log\"), \n  data = disc\n)\nmodel_parameters(fit2)\nmodel_parameters(fit2, exponentiate = TRUE)\nmodel_performance(fit2)\nplot(estimate_relation(fit2))\n\n# Advanced: Overdispersion and quasi-poisson\ncheck_overdispersion(fit2)\nfit3 <- glm(\n  count ~ poly(year, degree = 2), \n  family = quasipoisson(link = \"log\"), \n  data = disc\n)\nmodel_parameters(fit3)\nmodel_parameters(fit3, exponentiate = TRUE)\nplot(estimate_relation(fit3))\n\n# Compare parameters between poisson and quasi-poisson\ncompare_parameters(fit2, fit3, select = \"ci_p2\")\n```\n:::\n\n\n# Multilevel (Mixed Effects) Modeling\n\n## Conceptual Overview {.smaller}\n\n- LM and GLM both assume [independent residuals]{.b .blue}\n    - This is violated by clustered and longitudinal data\n    - The estimated SEs will be **biased** (usually too small)\n\n::: {.fragment}\n- [Clustered data]{.b .green} has a higher level of organization\n    - *e.g., students from same classroom are more similar*\n    - *e.g., participants from same country are more similar*\n    \n:::\n\n::: {.fragment}\n- [Longitudinal data]{.b .green} is repeated sampled from same source\n    - *e.g., each participant responds to multiple trials*\n    - *e.g., the global temperature is measured every week*\n\n:::\n\n## Simpson's Paradox {.smaller}\n\nReaching the right conclusion may require knowing the clusters...\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3B_Slides_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n## Simpson's Paradox {.smaller}\n\nHere it looks like having **worse** symptoms predicts a ***better*** prognosis...\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3B_Slides_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n## Simpson's Paradox {.smaller}\n\nBut this data actually comes from three different hospitals!\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3B_Slides_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n## Simpson's Paradox {.smaller}\n\nWithin each hospital, the relationship is the opposite!\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3B_Slides_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n## Levels of Clustering {.smaller}\n\n- We can think and talk about clustered data as having [multiple levels]{.b .blue} (hence MLM)\n\n::: {.fragment}\n- We number levels from $1$ to $M$ where lower numbers represent more specific levels\n    - *e.g., Data is collected from students (L1) nested within classrooms (L2)*\n    - *e.g., Data is collected from students (L1) in classrooms (L2) in schools (L3)*\n    - *e.g., Data is collected from timepoints (L1) nested within participants (L2)*\n    - *e.g., Data is collected from timepoints (L1) in participants (L2) in countries (L3)*\n\n:::\n\n::: {.fragment}\n- What is and isn't considered a level?\n    - At each level, we are sampling from a broader population\n    - Thus, levels need to be *random* variables (and are usually discrete)\n    - *If we repeated the study, would the variable's values be the same?*\n    \n:::\n\n## Variables at Different Levels\n\n- We can have variables that vary at (or describe) each level\n    - L1: each **student**'s `age`, `sex`, and `test_score`\n    - L2: each **classroom**'s `subject` and `n_students`\n    - L3: each **school**'s `budget` and `charter_status`\n\n::: {.fragment}\n- In standard MLM, the outcome/DV has to be on L1\n    - But we can still use higher level predictors\n    - We can even have cross-level interactions\n\n:::\n\n## Cluster-Robust Standard Errors\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(sandwich)\nmlmath <- read_csv(\"mlmath.csv\")\nmlmath\n\nmlmath |> count(school)\nmlmath |> count(public)\n\n# Fit LM ignoring clustering within schools\n\nfit <- lm(\n  formula = math ~ 1 + homework,\n  data = mlmath\n)\nmodel_parameters(fit)\nmodel_performance(fit)\nestimate_relation(fit) |> plot()\n\n# Estimate cluster-robust standard errors\n\nmodel_parameters(\n  fit, \n  vcov = \"vcovCL\", \n  vcov_args = list(type = \"HC1\", cluster = mlmath$school)\n)\nestimate_relation(\n  fit,\n  vcov = \"vcovCL\", \n  vcov_args = list(type = \"HC1\", cluster = mlmath$school)\n) |> \n  plot()\n```\n:::\n\n\n## Building a Multilevel Model\n\n## Longitudinal Considerations\n\n\n",
    "supporting": [
      "3B_Slides_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}